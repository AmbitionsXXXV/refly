import { AIMessageChunk, BaseMessage } from '@langchain/core/messages';
import { START, END, StateGraphArgs, StateGraph } from '@langchain/langgraph';

// schema
import { z } from 'zod';
// types
import { SystemMessage } from '@langchain/core/messages';
import { HumanMessage } from '@langchain/core/messages';
import { Runnable, RunnableConfig } from '@langchain/core/runnables';
import { BaseSkill, BaseSkillState, SkillRunnableConfig, baseStateGraphArgs } from '../base';
import { ToolMessage } from '@langchain/core/messages';
import { pick, safeParseJSON } from '@refly/utils';
import {
  Icon,
  Resource,
  Note,
  Collection,
  SkillInvocationConfig,
  SkillMeta,
  SkillTemplateConfigSchema,
} from '@refly/openapi-schema';
import { ToolCall } from '@langchain/core/dist/messages/tool';
import { randomUUID } from 'node:crypto';
import { createSkillInventory } from '../inventory';
// tools
import { ReflyDefaultResponse } from '../tools/default-response';

interface GraphState extends BaseSkillState {
  /**
   * Accumulated messages.
   */
  messages: BaseMessage[];
  /**
   * Skill calls to run.
   */
  skillCalls: ToolCall[];
  contextualUserQuery: string; // 基于上下文改写 userQuery
}

export class Scheduler extends BaseSkill {
  name = 'scheduler';

  displayName = {
    en: 'Knowledge Curator',
    'zh-CN': '知识管家',
  };

  icon: Icon = { type: 'emoji', value: '🧙‍♂️' };

  configSchema: SkillTemplateConfigSchema = {
    items: [],
  };

  invocationConfig: SkillInvocationConfig = {};

  description = "Inference user's intent and run related skill";

  schema = z.object({
    query: z.string().optional().describe('The search query'),
  });

  graphState: StateGraphArgs<GraphState>['channels'] = {
    ...baseStateGraphArgs,
    messages: {
      reducer: (x: BaseMessage[], y: BaseMessage[]) => x.concat(y),
      default: () => [],
    },
    skillCalls: {
      reducer: (x: ToolCall[], y: ToolCall[]) => y, // always update with newer value
      default: () => [],
    },
    contextualUserQuery: {
      reducer: (left?: string, right?: string) => (right ? right : left || ''),
      default: () => '',
    },
  };

  // Default skills to be scheduled (they are actually templates!).
  skills: BaseSkill[] = createSkillInventory(this.engine);

  // Scheduler config snapshot, should keep unchanged except for `spanId`.
  configSnapshot?: SkillRunnableConfig;

  isValidSkillName = (name: string) => {
    return this.skills.some((skill) => skill.name === name);
  };

  directCallSkill = async (state: GraphState, config: SkillRunnableConfig): Promise<Partial<GraphState>> => {
    const { selectedSkill, installedSkills } = config.configurable || {};

    const skillInstance = installedSkills.find((skill) => skill.skillId === selectedSkill.skillId);
    if (!skillInstance) {
      throw new Error(`Skill ${selectedSkill.tplName} not installed.`);
    }

    const skillTemplate = this.skills.find((tool) => tool.name === selectedSkill.tplName);
    if (!skillTemplate) {
      throw new Error(`Skill ${selectedSkill} not found.`);
    }

    const skillConfig: SkillRunnableConfig = {
      ...config,
      configurable: {
        ...config.configurable,
        currentSkill: skillInstance,
      },
    };

    this.emitEvent({ event: 'start' }, skillConfig);
    const output = await skillTemplate.invoke({ query: state.query }, skillConfig);

    // We'll send end event in genRelatedQuestions node.
    // So don't send it here.

    const message = new AIMessageChunk({
      name: skillTemplate.name,
      content: typeof output === 'string' ? output : JSON.stringify(output),
    });

    return { messages: [message] };
  };

  genToolMsgSummarization = async (needSummarizedContent: string) => {
    const getSystemPrompt = (
      needSummarizedContent: string,
    ) => `You will be provided with a result generated by a tool. Your task is to summarize the most essential information from these results. The summary should include all key points and be no more than 100 words.

Tool results are provided within triple quotes.
"""
${needSummarizedContent}
"""

Summary requirements:
1. The summary must include all key points;
2. Important: The word limit is **100 words**.

After completing the summary, please provide suggestions for the next decision-making steps.

Example tool results:
"""
- The key is on the table.
- The door lock is broken and needs a technician to repair it.
- The living room light isn't working, possibly due to a faulty bulb.
- Schedule a repair for the door lock and bulb replacement as soon as possible.
"""

Example summary:
"""
The key is on the table. The door lock is broken and requires a technician. The living room bulb is faulty and needs replacement. Schedule the repairs and bulb replacement promptly.
"""

Please generate the summary based on these requirements and offer suggestions for the next steps.
  `;

    const model = this.engine.chatModel({ temperature: 0.1, maxTokens: 100 });

    const runnable = model.withStructuredOutput(
      z
        .object({
          summary: z.string(),
        })
        .describe(`Generate the summary based on these requirements and offer suggestions for the next steps.`),
    );
    const summaryModelRes = await runnable.invoke([new HumanMessage(getSystemPrompt(needSummarizedContent))]);

    return summaryModelRes?.summary || '';
  };

  getToolMsg = async (currentSkill: SkillMeta, query: string, realOutput: { messages: BaseMessage[] }) => {
    let realToolOutputMsg: ToolMessage;

    let toolSuccessMsgTemplate = `The **${currentSkill.tplName}** tool is already completed the task based on given user query: **${query}**.
    ## Tool result
      Tool result are provided within triple quotes.
      """
      {{toolResult}}
      """ 
    ## Note
    - The result is **already send to user**. 
    - Please evaluate whether the user's request has been fully satisfied. If further actions are needed, determine the next appropriate tool to call; otherwise, terminate the response.`;
    const toolFailedMsgTemplate = `The **${currentSkill.tplName}** tool call without any content, please check whether need call new tool or stop response`;

    // handle summarize for tool operator
    if (realOutput?.messages?.length > 0) {
      const lastMsg = realOutput.messages[realOutput.messages.length - 1];
      let realToolOutputMsgContent = '';

      if (lastMsg?.content?.length > 0) {
        const summarizedToolMsg = await this.genToolMsgSummarization(
          typeof lastMsg.content === 'string' ? lastMsg.content : JSON.stringify(lastMsg.content),
        );
        realToolOutputMsgContent = toolSuccessMsgTemplate.replace('{{toolResult}}', summarizedToolMsg);
      } else {
        realToolOutputMsgContent = toolFailedMsgTemplate;
      }

      realToolOutputMsg = new ToolMessage({
        name: currentSkill.tplName,
        content: realToolOutputMsgContent,
        tool_call_id: currentSkill?.skillId!,
      });
    } else {
      realToolOutputMsg = new ToolMessage({
        name: currentSkill.tplName,
        content: toolFailedMsgTemplate,
        tool_call_id: currentSkill?.skillId!,
      });
    }

    return realToolOutputMsg;
  };

  /**
   * Call the first scheduled skill within the state.
   */
  callSkill = async (state: GraphState, config: SkillRunnableConfig): Promise<Partial<GraphState>> => {
    const { skillCalls, query, contextualUserQuery } = state;
    if (!skillCalls) {
      this.emitEvent({ event: 'log', content: 'No skill calls to proceed.' }, config);
      return {};
    }

    const { locale = 'en' } = config.configurable || {};

    // Pick the first skill to call
    const call = state.skillCalls[0];

    // We'll first try to use installed skill instance, if not found then fallback to skill template
    const { installedSkills = [] } = config.configurable || {};
    const skillInstance = installedSkills.find((skill) => skill.tplName === call.name);
    const skillTemplate = this.skills.find((skill) => skill.name === call.name);
    const currentSkill: SkillMeta = skillInstance ?? {
      tplName: skillTemplate.name,
      displayName: skillTemplate.displayName[locale],
      icon: skillTemplate.icon,
    };
    const skillConfig: SkillRunnableConfig = {
      ...config,
      configurable: {
        ...config.configurable,
        currentSkill,
        spanId: randomUUID(), // generate new spanId for each managed skill call
      },
    };

    this.emitEvent({ event: 'start' }, skillConfig);

    // Dequeue the first skill call from the state
    let result: Partial<GraphState> = {
      skillCalls: state.skillCalls.slice(1),
    };

    try {
      const output = await skillTemplate.invoke(call.args, skillConfig);
      const realOutput: { messages: BaseMessage[] } = typeof output === 'string' ? safeParseJSON(output) : output;
      const realToolOutputMsg = await this.getToolMsg(
        {
          tplName: currentSkill.tplName,
          skillId: call?.id,
          displayName: currentSkill.displayName,
          icon: currentSkill.icon,
        },
        contextualUserQuery || query,
        realOutput,
      );

      result = { messages: [realToolOutputMsg] };
    } catch (error) {
      this.engine.logger.error(`Error calling skill ${currentSkill.tplName}: ${error.stack}`);
    } finally {
      this.emitEvent({ event: 'end' }, skillConfig);
    }

    return result;
  };

  private async recognizeUserIntent(
    query: string,
    context: {
      contentList: string[];
      resources: Resource[];
      notes: Note[];
      collections: Collection[];
      messages: BaseMessage[];
    },
  ): Promise<string> {
    this.emitEvent({ event: 'log', content: 'Recognizing user intent...' }, this.configSnapshot);
    /**
     * 基于给定上下文，和聊天历史，实现最有效的判断意图的能力，撰写 Prompt，调用 LLM
     *
     * 1. 给定上下文和聊天历史
     *  1. 上下文：contentList <string[]>、resources <Resource[]>、notes <Note[]>、collections <Collection[]>
     *  2. 聊天历史：messages <BaseMessage[]>
     * 2. 待识别用户意图：
     *  1. 搜索问答，比如基于给定的上下文回答问题（知识库搜索，或者用户明确表明了需要进行在线搜索）
     *  2. 基于上下文进行写作，比如写邮件、写博客、优化表达、续写、缩写等
     *  3. 基于上下文进行阅读理解，比如总结、解释、翻译
     * 3. 上下文可以拼接元信息，比如资源、笔记、集合的元信息，传入 Prompt 辅助意图识别
     *
     */
    const { contentList, resources, notes, collections, messages } = context;

    const getSystemPrompt =
      () => `You are an advanced AI assistant specializing in recognizing user intents. Your task is to analyze the given query and context to determine the user's primary intent.

Possible intents:
1. SEARCH_QA: The user is asking a question that requires searching through given context or explicitly requests online search.
2. WRITING: The user wants help with writing tasks such as composing emails, blog posts, optimizing expressions, continuing text, or summarizing.
3. READING_COMPREHENSION: The user needs help understanding, summarizing, explaining, or translating given text.

Context Information:
- Content List: ${contentList.length} items
- Resources: ${resources.length} items
- Notes: ${notes.length} items
- Collections: ${collections.length} items
- Chat History: ${messages.length} messages

Guidelines:
1. Analyze the query and all provided context carefully.
2. Consider the nature of the query and how it relates to the available context.
3. Pay attention to specific keywords or phrases that might indicate a particular intent.
4. Take into account the types and amount of context provided (e.g., many resources might suggest a search task).
5. Consider the chat history for any relevant context that might influence the intent.

Output your response in the following JSON format:
{
  "intent": "SEARCH_QA | WRITING | READING_COMPREHENSION",
  "confidence": 0.0 to 1.0,
  "reasoning": "A brief explanation of your reasoning"
}`;

    const getUserMessage = () => `Query: ${query}

Context Summary:
${this.summarizeContext(context)}

Please analyze the query and context to determine the user's primary intent.`;

    const model = this.engine.chatModel({ temperature: 0 });
    const runnable = model.withStructuredOutput(
      z.object({
        intent: z.enum(['SEARCH_QA', 'WRITING', 'READING_COMPREHENSION']),
        confidence: z.number().min(0).max(1),
        reasoning: z.string(),
      }),
    );

    const result = await runnable.invoke([new SystemMessage(getSystemPrompt()), new HumanMessage(getUserMessage())]);

    this.engine.logger.log(`Recognized intent: ${result.intent} (confidence: ${result.confidence})`);
    this.engine.logger.log(`Intent recognition reasoning: ${result.reasoning}`);

    this.emitEvent(
      { event: 'log', content: `Recognized intent: ${result.intent} (confidence: ${result.confidence})` },
      this.configSnapshot,
    );
    this.emitEvent({ event: 'log', content: `Intent recognition reasoning: ${result.reasoning}` }, this.configSnapshot);
    return result.intent;
  }

  private summarizeContext(context: {
    contentList: string[];
    resources: Resource[];
    notes: Note[];
    collections: Collection[];
    messages: BaseMessage[];
  }): string {
    const { contentList, resources, notes, collections, messages } = context;

    const summarizeResources = (resources: Resource[]) =>
      resources.map((r) => `- ${r.resourceType}: "${r.title}" (ID: ${r.resourceId})`).join('\n');

    const summarizeNotes = (notes: Note[]) => notes.map((n) => `- Note: "${n.title}" (ID: ${n.noteId})`).join('\n');

    const summarizeCollections = (collections: Collection[]) =>
      collections.map((c) => `- Collection: "${c.title}" (ID: ${c.collectionId})`).join('\n');

    const summarizeMessages = (messages: BaseMessage[]) =>
      messages.map((m) => `- ${m._getType()}: ${(m.content as string)?.substring(0, 50)}...`).join('\n');

    return `Content List:
  ${contentList.map((c, i) => `- Content ${i + 1}: ${c.substring(0, 50)}...`).join('\n')}
  
  Resources:
  ${summarizeResources(resources)}
  
  Notes:
  ${summarizeNotes(notes)}
  
  Collections:
  ${summarizeCollections(collections)}
  
  Recent Messages:
  ${summarizeMessages(messages.slice(-5))}`;
  }

  private async optimizeQuery(
    query: string,
    context: {
      locale: string;
      chatHistory: BaseMessage[];
    },
  ): Promise<{
    decomposedQueries: Array<{ subquery: string; relevance: number }>;
    optimizedQueries: Array<{ query: string; score: number; reasoning: string }>;
    translations: Array<{ language: string; query: string }>;
  }> {
    /**
     * 1. 基于给定的 query、locale 和 chatHistory，优化用户的 query 为多种变体（拆分query，改写、补充完整等）、多语种
     * 2. 返回优化后的 query 集合和理由分数等
     * 3. 撰写 Prompt，调用 LLM
     *
     */
    const { locale, chatHistory } = context;

    const getSystemPrompt =
      () => `You are an advanced AI assistant specializing in query optimization, decomposition, and translation. Analyze the given query, considering the user's locale and chat history, to provide optimized and decomposed versions of the query along with translations.
  
  Guidelines:
  1. Decompose the original query into 3-6 subqueries, each addressing a specific aspect of the main query.
  2. Generate 3 optimized versions of the original query, each addressing a different aspect or interpretation.
  3. For each optimized query and subquery, provide a relevance score (0.0 to 1.0) and a brief reasoning.
  4. Translate the original query into English (if not already in English) and one other relevant language.
  5. Consider the chat history for context that might influence query optimization and decomposition.
  6. Ensure that optimized queries and subqueries maintain the original intent while potentially expanding or clarifying it.
  
  User's locale: ${locale}
  
  Output your response in the following JSON format:
  {
    "decomposedQueries": [
      {
        "subquery": "Decomposed subquery 1",
        "relevance": 0.0 to 1.0
      },
      // ... (3-6 subqueries)
    ],
    "optimizedQueries": [
      {
        "query": "Optimized query 1",
        "score": 0.0 to 1.0,
        "reasoning": "Brief explanation for this optimization"
      },
      // ... (3 optimized queries)
    ],
    "translations": [
      {
        "language": "English",
        "query": "Translated query in English"
      },
      {
        "language": "Another relevant language",
        "query": "Translated query in another language"
      }
    ]
  }`;

    const getUserMessage = () => `Original Query: ${query}
  
  Chat History Summary:
  ${this.summarizeChatHistory(chatHistory)}
  
  Please decompose, optimize the query, and provide translations based on the given guidelines.`;

    const model = this.engine.chatModel({ temperature: 0.3 });
    const runnable = model.withStructuredOutput(
      z.object({
        decomposedQueries: z.array(
          z.object({
            subquery: z.string(),
            relevance: z.number().min(0).max(1),
          }),
        ),
        optimizedQueries: z.array(
          z.object({
            query: z.string(),
            score: z.number().min(0).max(1),
            reasoning: z.string(),
          }),
        ),
        translations: z.array(
          z.object({
            language: z.string(),
            query: z.string(),
          }),
        ),
      }),
    );

    const result = await runnable.invoke([new SystemMessage(getSystemPrompt()), new HumanMessage(getUserMessage())]);

    this.engine.logger.log(
      `Query optimized with ${result.decomposedQueries.length} subqueries, ${result.optimizedQueries.length} variations, and ${result.translations.length} translations`,
    );
    this.engine.logger.log(`Query optimization result: ${JSON.stringify(result, null, 2)}`);

    return {
      decomposedQueries:
        result.decomposedQueries?.map((q) => ({
          subquery: q.subquery || '',
          relevance: q.relevance || 0,
        })) || [],
      optimizedQueries:
        result.optimizedQueries?.map((q) => ({
          query: q.query || '',
          score: q.score || 0,
          reasoning: q.reasoning || '',
        })) || [],
      translations:
        result.translations?.map((t) => ({
          language: t.language || '',
          query: t.query || '',
        })) || [],
    };
  }

  private summarizeChatHistory(chatHistory: BaseMessage[]): string {
    // Take the last 5 messages for context
    const recentMessages = chatHistory.slice(-5);
    return recentMessages.map((msg) => `${msg._getType()}: ${(msg.content as string)?.substring(0, 50)}...`).join('\n');
  }

  callScheduler = async (state: GraphState, config: SkillRunnableConfig): Promise<Partial<GraphState>> => {
    const { query, contextualUserQuery, messages = [] } = state;

    this.configSnapshot ??= config;
    this.emitEvent({ event: 'start' }, this.configSnapshot);

    const { locale = 'en', chatHistory = [], installedSkills, currentSkill, spanId } = this.configSnapshot.configurable;

    // 目前先不支持选技能，未来支持有限制的选技能
    // let tools = this.skills;
    // if (installedSkills) {
    //   const toolMap = new Map(tools.map((tool) => [tool.name, tool]));
    //   tools = installedSkills.map((skill) => toolMap.get(skill.tplName)!).filter((tool) => tool);
    // }

    // tools = tools.filter((tool) => tool);
    // const boundModel = this.engine
    //   .chatModel()
    //   .bindTools([...tools, new ReflyDefaultResponse()], { parallel_tool_calls: false });

    const criticalGuidelines = `## Critical Guidelines

      ### 1. Minimum Tools Principle:
          • Use as few tools as possible to solve the user's problem.
          • Prioritize the most relevant tool when multiple options are available.
          • If the user's needs are satisfied, conclude the task immediately.
      
      ### 2. Sequential Tool Invocation:
          • Emphasize sequential invocation of tools—only call one tool at a time.
          • Adhere to the "minimum viable principle" by using only what is necessary to achieve the desired outcome.
          
      ### 3. Task Completion:
          • Use tool results solely to determine whether to continue or end the task.
          • Do not disclose or output any tool execution details. If the task is complete, indicate completion without additional content.
          • Assess whether the available tool can resolve the user's request. If the tool does not match the user's needs, invoke the **default_response** tool, or do not call any tool. Avoid invoking irrelevant tools. For example:
            - If a user asks for a current event that requires browsing, but browsing is unavailable, respond using **default_response** instead of invoking unrelated tools.
            - If a user's query involves an unfamiliar term that does not match any tool's capabilities, avoid tool invocation and provide a response based on common knowledge or the **default_response** tool.`;

    const getSystemPrompt = (locale: string) => `## Role
      You are an AI intelligent response engine built by Refly AI that specializes in selecting the most suitable tools from a variety of options based on user requirements.
      
      ## Skills
      ### Skill 1: Analyzing User Intent
      - Identify key phrases and words from the user's questions.
      - Understand the user's requests based on these key elements.
      
      ### Skill 2: Optimizing Suitable Tools
      - Select the most appropriate tool(s) from the tool library to address the user's needs.
      - If there are multiple similar tools capable of addressing the user's issue, ask the user for additional clarification and return an optimized solution based on their response.
      
      ### Skill 3: Step-by-Step Problem Solving
      - If the user's requirements need multiple tools to be processed step-by-step, optimize and construct the tools sequentially based on the intended needs.
      - Ensure that only one tool is called at a time in the sequence, adhering to the "minimum viable principle."
      
      ### Skill 4: Direct Interaction
      - If the tool library cannot address the issues, rely on your internal common knowledge to interact and communicate directly with the user.
      
      ## Constraints
      - Some tools may have concise or vague descriptions; detailed reasoning and careful selection of the most suitable tool based on user needs are required.
      - Only address and guide the creation or optimization of relevant issues; do not respond to unrelated user questions.
      - Assess the suitability of available tools before invocation. If a tool is not suitable, invoke the **default_response** tool or no tool at all. Avoid invoking irrelevant tools.
      - Always respond in the locale **${locale}** language.
      - Provide the optimized guidance immediately in your response without needing to explain or report it separately.
      
      ${criticalGuidelines}
      `;

    const responseMessage = await boundModel.invoke(
      [
        new SystemMessage(getSystemPrompt(locale)),
        ...chatHistory,
        ...messages,
        new HumanMessage(`The user's intent is ${contextualUserQuery || query} \n\n ${criticalGuidelines}`),
      ],
      {
        ...this.configSnapshot,
        metadata: {
          ...this.configSnapshot.metadata,
          ...currentSkill,
          spanId,
        },
      },
    );
    const { tool_calls: skillCalls } = responseMessage;

    // hanlde default response
    const hasAnyToolCall = messages.find((message) => (message as ToolMessage)?._getType() === 'tool');

    if (skillCalls.length > 0) {
      const skillCall = skillCalls[0];
      if (!hasAnyToolCall && skillCall.name === 'default_response') {
        return await this.commonSenseGenerate(state, config, false);
      }

      this.emitEvent(
        {
          event: 'log',
          content: `Decide to call skills: ${skillCalls.map((call) => call.name).join(', ')}`,
        },
        this.configSnapshot,
      );
      this.emitEvent({ event: 'end' }, this.configSnapshot);

      // Regenerate new spanId for the next scheduler call.
      this.configSnapshot.configurable.spanId = randomUUID();
    } else {
      if (messages?.length === 0 || !hasAnyToolCall) {
        return await this.commonSenseGenerate(state, config, false);
      }
    }

    return { messages: [responseMessage], skillCalls };
  };

  commonSenseGenerate = async (
    state: GraphState,
    config: SkillRunnableConfig,
    callByGraph = true,
  ): Promise<Partial<GraphState>> => {
    const { messages = [], query, contextualUserQuery } = state;

    this.configSnapshot ??= config;

    // default by langgraph engine call, but can be called as function
    if (callByGraph) {
      this.emitEvent({ event: 'start' }, this.configSnapshot);
    }

    const {
      contentList = [],
      locale = 'en',
      chatHistory = [],
      currentSkill,
      spanId,
    } = this.configSnapshot.configurable; // scheduler only handle contentList when no skill could

    // without any skill, scheduler can handle contentList for common knowledge q & a
    const getSystemPrompt = (locale: string) => `- Role: Knowledge Management Assistant
- Background: Users require an intelligent assistant capable of understanding queries and context information, even when the context is absent, to provide answers in the language of the query while maintaining the language of professional terms.
- Profile: You are an AI developed by Refly AI, specializing in knowledge management, adept at reading, writing, and integrating knowledge, and skilled in providing responses in the language of the user's query.
- Skills: You possess capabilities in text parsing, information extraction, knowledge association, intelligent Q&A, and the ability to generate context from a query when necessary.
- Goals: Provide accurate, relevant, and helpful answers based on the user's query and available context information, ensuring that the language of the response matches the query and that professional terms are maintained in their original language.
- Constrains:
  1. Always respond in the language of the user's query, the user's locale is ${locale}.
  2. Maintain professional terms in their original language within the response.
  3. Ensure the response is clear and understandable, even with the inclusion of professional terms.
- OutputFormat: Clear, accurate, and helpful text responses in the query's language, with professional terms in their original language.
- Workflow:
  1. Receive the user's query and any provided context information.
  2. Analyze the query and context information, or generate context if necessary, to extract key points.
  3. Generate accurate and relevant answers, ensuring the response language matches the query and professional terms are in their original language.
- Examples:
  - Example 1: Query: "What is artificial intelligence?" Context: ["Artificial intelligence is a technology that simulates human intelligence", "Artificial intelligence can perform a variety of tasks"]
    Answer: "Artificial intelligence is a technology that simulates human intelligence and can perform a variety of tasks, such as recognizing language and solving problems."
  - Example 2: Query: "How to improve work efficiency?" Context: ["Using tools can improve work efficiency", "Proper time planning is also important"]
    Answer: "Improving work efficiency can be achieved by using appropriate tools and proper time planning."
  - Example 3: Query: "Define sustainability" Context: []
    Answer: "Sustainability refers to the ability to maintain a certain process or state without depleting resources or causing long-term harm to the environment, economy, or society."
- Initialization: In the first conversation, please directly output the following: Hello, I am your Knowledge Management Assistant. I can help you answer queries and provide answers based on context information, even if the context is not provided. My responses will always be in the language of your query, and I will maintain the original language of professional terms. Please tell me your query and any relevant context information you have.
  `;
    const getUserPrompt = (query: string, contentList: string[]) => {
      return `Query: ${contextualUserQuery || query} \n\n Context: [${contentList.filter((item) => item).join(', ')}]`;
    };

    const model = this.engine.chatModel({ temperature: 0.5 });

    const responseMessage = await model.invoke(
      [
        new SystemMessage(getSystemPrompt(locale)),
        ...chatHistory,
        ...messages,
        new HumanMessage(
          getUserPrompt(
            query,
            contentList.map((item) => item.content),
          ),
        ),
      ],
      {
        ...this.configSnapshot,
        metadata: {
          ...this.configSnapshot.metadata,
          ...currentSkill,
          spanId,
        },
      },
    );

    return { messages: [responseMessage], skillCalls: [] };
  };

  genRelatedQuestions = async (state: GraphState, config: SkillRunnableConfig) => {
    const { messages = [] } = state;
    const { locale = 'en', selectedSkill } = config.configurable || {};

    const skillConfig = selectedSkill
      ? {
          ...config,
          configurable: {
            ...config.configurable,
            currentSkill: selectedSkill,
          },
        }
      : this.configSnapshot;

    const getSystemPrompt = (locale: string) => `## Role
You are an SEO (Search Engine Optimization) expert, skilled at identifying key information from the provided context and proposing three semantically relevant recommended questions based on this information to help users gain a deeper understanding of the content.

## Skills

### Skill 1: Context Identification
- Understand and analyze the given context to determine key information.

### Skill 2: Recommending Questions
- Propose three questions that best fit the context's semantics based on key information, to assist users in better understanding the content.
- Format example:
=====
   - ❓ Recommended Question 1: <Question 1>
   - ❓ Recommended Question 2: <Question 2>
   - ❓ Recommended Question 3: <Question 3>
=====

## Emphasis

- Questions should be **short, concise, and contextual**

Generated question example:

- What are some common English phrases used in button copy for internet products?
- How can I write effective button copy in English for my internet product?
- What are some best practices for writing button copy in English for internet products?

> Up is only for examples, please output related questions in locale: ${locale} language

## Limitations:
- Only propose questions and answers related to the context.
- Strictly adhere to the provided output format.
- Always provide answers that match the user's query.
- Begin the answer directly with the optimized prompt.
  `;

    const model = this.engine.chatModel({ temperature: 0.1 });

    const runnable = model.withStructuredOutput(
      z
        .object({
          recommend_ask_followup_question: z
            .array(z.string())
            .describe(`Generate three recommended follow-up questions in locale: ${locale} language`),
        })
        .describe(
          `Understand and analyze the provided context to identify key information, and based on this ` +
            `key information, formulate three questions that best align with the context's semantics ` +
            `to assist users in gaining a better understanding of the content.`,
        ),
    );

    try {
      const askFollowUpQuestion = await runnable.invoke([
        new SystemMessage(getSystemPrompt(locale)),
        ...messages,
        new HumanMessage(`Please output answer in ${locale} language:`),
      ]);

      const followUps = askFollowUpQuestion?.recommend_ask_followup_question || [];

      this.emitEvent(
        {
          event: 'structured_data',
          content: JSON.stringify(followUps),
          structuredDataKey: 'relatedQuestions',
        },
        skillConfig,
      );
    } catch (error) {
      // Models can sometimes fail to return structured data, so we just log it and do nothing
      this.engine.logger.error(`Error generating related questions: ${error.stack}`);
    } finally {
      this.emitEvent({ event: 'end' }, skillConfig);
    }

    return {};
  };

  shouldDirectCallSkill = (
    state: GraphState,
    config: SkillRunnableConfig,
  ): 'direct' | 'scheduler' | 'commonSenseGenerate' => {
    const { selectedSkill, installedSkills = [] } = config.configurable || {};

    if (!selectedSkill) {
      return 'scheduler';
    }

    if (!this.isValidSkillName(selectedSkill.tplName)) {
      this.emitEvent(
        {
          event: 'log',
          content: `Selected skill ${selectedSkill.tplName} not found. Fallback to scheduler.`,
        },
        config,
      );
      return 'scheduler';
    }

    return 'direct';
  };

  shouldCallSkill = (state: GraphState, config: SkillRunnableConfig): 'skill' | 'relatedQuestions' | typeof END => {
    const { skillCalls = [] } = state;
    const { convId } = this.configSnapshot?.configurable ?? config.configurable;

    if (skillCalls.length > 0) {
      return 'skill';
    }

    // If there is no skill call, then jump to relatedQuestions node
    return convId ? 'relatedQuestions' : END;
  };

  onDirectSkillCallFinish = (state: GraphState, config: SkillRunnableConfig): 'relatedQuestions' | typeof END => {
    const { convId } = config.configurable || {};

    // Only generate related questions in a conversation
    return convId ? 'relatedQuestions' : END;
  };

  onSkillCallFinish(state: GraphState, config: SkillRunnableConfig): 'scheduler' | 'skill' {
    const { skillCalls } = state;

    // Still have skill calls to run
    if (skillCalls.length > 0) {
      return 'skill';
    }

    // All skill calls are finished, so we can return to the scheduler
    return 'scheduler';
  }

  toRunnable(): Runnable<any, any, RunnableConfig> {
    const workflow = new StateGraph<GraphState>({
      channels: this.graphState,
    })
      .addNode('direct', this.directCallSkill)
      .addNode('scheduler', this.callScheduler)
      .addNode('commonSenseGenerate', this.commonSenseGenerate)
      .addNode('skill', this.callSkill)
      .addNode('relatedQuestions', this.genRelatedQuestions);

    workflow.addConditionalEdges(START, this.shouldDirectCallSkill);
    workflow.addConditionalEdges('direct', this.onDirectSkillCallFinish);
    workflow.addConditionalEdges('commonSenseGenerate', this.onDirectSkillCallFinish);
    workflow.addConditionalEdges('scheduler', this.shouldCallSkill);
    workflow.addConditionalEdges('skill', this.onSkillCallFinish);
    workflow.addEdge('relatedQuestions', END);

    return workflow.compile();
  }
}
